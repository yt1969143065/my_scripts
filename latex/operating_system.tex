\documentclass{book}
\newcommand {\kw}  [1] {\textbf{#1}}
\newcommand {\www} [1] {\texttt{#1}}
\newcommand {\sys} [1] {\textsl{#1}}
\newcommand {\cmd} [1] {\textsl{#1}}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@Preface

Most books on operating systems are strong on thoery and weak on practice. 
This one aims to provide a better balance between the two.
It covers al the fundamental principles in great detail, 
including processes, interprocess communication, semaphores, monitors, message passing, 
scheduling algorithms, input/output, deadlocks, device driver, memory management, paging algorithms,
file system design, security, and protection mechanisms.
But it also discusses one particular system MINIX 3a UNIX-compatible operating system in detail, 
and even provides a source code listing for study.
This arrangement allows the reader not only to learn the principles, but also to see how they are applied in a real operating system.

When the first edition of this book appeared in 1987, it caused something of a small revolution in the way operating systems coursed were taight.
Untill then, most coursed just covered theory.
With the appearance of MINIX, many schools began to have laboratory courses in which students examined a real operating system to see how it worked inside.
We consoder this trend highly desirable and hope it continues.

It its first 10 years, MINIX underwent many changes.
The original code was designed for a 256K 8088-based IBM PC with two diskette drivers and no hard disk.
It was also based on UBIX Version 7. 
As time went on, MINIX evolved in many ways: it supported 32-bit protected mode machined with large memories and hard disks.
It also changed from being based on Version 7, to being based on the international POSIX standard (IEEE 1003.1 and ISO 9945-1).
Finally, many new features were added, perhaps too many in our view, but too few in the view of some other people, which led to the creation of Linux.
In addition, MINIX was ported to many other platforms, including the Macintosh, Amiga, Atari, and SPARC.
A second edition of the book, covering this system, was published in 1997 and was widely used at universities.

The popularity of MINIX has continued, as can be observed by examining the number of hits for MINIX found by Google.
 
This third edition of the book has many changes throughout.
Nearly all of the material on principles has been revised, and considerable new material has been added.
However, the main change is the discussion of the new version of the system, called MINIX 3, and the inclusion of the new code in this book.
Although loosely based on MINIX 2, MINIX 3is fundamentally different in many key ways.

The design of MINIX 3 was inspired by the observation that operating system are becoming bloated, slow, and unreliable. 
They crash far more often than other electronic devices such as televisons, cell phones, and DVD players 
and have so many features and options that practically nobody can understand them fully or manage them well.
And of cource, computer viruses, worms, spyware, spam, and other forms of ,alware have become epidemic.

To a large extent, many of these problems are caused by a fundamental design flaw in current operating systems: their lack of modularity. 
The entire operating system is typically millions of lines of C/C++ code compiled into a single massive executable program run in kernrl mode.
A bug in any one of those millions of lines of code can cause the system to malfunction.
Getting all this code correct is impossible, especially when about 70\% consists of device drivers, written by third parties, 
and outside the purview of the people maintaining the operating system.

With MINIX 3, we demonstrate that this monolithic design is not the only possibility. 
The MINIX 3 kernel is only about 4000 lines of executable code, not the millions found in Windows, Linux, Mac OSX, or FreeBSD.
The rest of the system, including all the device drivers(except the clock driver), is a collection of small, modular, user-mode processes, 
each of which is tightly restricted in what it can do and with which other processes it may communicate.

While MINIX 3 is a work in progress, we believe that this model of building an operating system as a collection of highly-encapsulated user-mode 
processes holds promise for building more reliable system in the future.
MINIX 3 is especially focused on smaller PCs
(such as those commonly found in Third-World countries and on embedded systems, which are always resource constrained).
In any event, this design makes it much easier for students to lear how an operating system works than attempting to study a huge monolithic system.

The CD-ROM that is included in this book is a live CD. 
You can put it in your CD-ROM drive, reboot the computer, and MINIX 3 will give a login prompt within a few seconds. 
You can log in as root and give the system a try without first having to install it on your hard disk.
Of course, it can also be installed on the hard disk.
Detailed installation instructions are given in Appendix A.

As suggested above, MINIX 3is rapidly evolving, with new versions being issued frequently.
To download the current CD-ROM image file for burning, please go to the offical Website: www.minix3.org.
This site also contains a large amount of new software, documentation, and news about MINIX 3 development.
For discussions about MINIX 3, or to ask questions, there is a USENET newsgroup: comp.os.minix.
People without newsreaders can follow discussions on the Web at http://groups.google.com/group/comp.os.minix.

As an alternative to installing MINIX 3 on your hard disk, it is possible to run it on any one of several PC simulators now available.
Some of these are listed on the main page of the Website.

Instructors who are using the book as the text for a university course can get the problem solutions from their local Prentice Hall representative. 
The book has ints own Website.
It can be found by going to www.prenhall.com/tanenbaum and selecting this title.

We have been extremely fortunate in having the help of many people during the course of this project.
First and doremost, Ben Gras and Jorrit Herder have done most of the programming of the new version.
They did a great job under tight time constraints, including responseding to e-mail well after midnight on many occasions. 
They also read the manuscript and made many useful comments.
Our deepest appreciation to both of them.

Kees Bot also helped greatly with previous versions, giving us a good base to work with.
Kees wrote large chunks of code for versions up to 2.0.4, repaired bugs, and answered numerous questions.
Philip Homburg wrote most of the networking code as well as helping out in numerous other useful ways, 
especially providing detailed feedback on the manuscript.

People too numerous to list contributed code to the every early versions, helping to get MINIX off the ground in the first place.
There were so many of them and their contributions have been so varied that we cannot even begin to list them all here,
so the best we can do is a generic thank you to all of them.

Several people read parts of the manuscript and made suggestions.
We would like to give our special thanks to Gojko Babic, Michael Crowley, Joseph M. Kizza, Sam Kohn Alexander Manov, and Du Zhang for their help.

Finally, we would like to thank our families. 
Suzanne has been through this 16 times now. 
Barbara has been through it 15 times now.
Marvin has been through it 14 times now.
It's kind of getting to be routine, but the love and support is still much appreciated.(AST)

AI's Barbara has been through this twice now.
Her support, patience, and good humor were essential.
Gordon has been a patient listenser.
It is still a delight to have a son who understands and cares about the things that fascinate me.
Finally, step-grandson Zain's first birsday coincides with the release of MINIX 3.
Some day he will appreciate this.(ASW)

Andrew S. Tanenbaum

Albert S. Woodhull


\chapter{Introduction}
Without its software, a computer is basically a useless lump of metal.
With its software, a computer can store , process, and retrieve information; 
play music and videos; send e-mail, search the Internet; and engage in many other valuable activities to earn its keep.
Computer software can be divided into two kinds: system programs, which manage the operation of the computer itself, 
and application programs, which perform the actual work the user wants.
The most fundamental system program is the \kw{operating system}, whose job is to control all the computer's resources and 
provide a base upon which the application programs can be written.
Operating systems are the topic of this book.
In particular, and operating system called MINIX 3 is used as a model, to illustrate design principles and the realities of implementing a design.

A modern computer system consists of one or more processors, some main memory, disks, 
printers, a keyboard, a display, network interfacees, and other input/output devices. 
All in all, a complex system.
Writing programs that keep track of all these components and use them correctly, let alone optimally, is an extremely difficult job.
If every programmer had to be concerned with how disk drives work, and with all the dozons of things that could go wrong when reading a disk block, 
it is unlikely that many programs could be written at all.

Many years ago it became abundantly clear that some way had to be found to shield programmers from the complexity of the hardware.
The way that has evolved gradually is to put a layer of software on top of the bare hardware, 
to manage all parts of the system, and present the user with an interface or \kw{virtual machine} that easier to understand and program.
This layer of software is the operating system.

The placement of the operating system is shown in Fiq. 1-1.
At the bottom is the hardware, which, in many cases, is itself composed of two or more levels (or layers).
The lowest level contains physical devices, consisting of integrated circuit chips, wires, power supplies, cathode ray tubes, and similar physical devices.
How these are constructed and how they work is the province of the electrical engineer.

Next comes the \kw{microarchitecture level}, in which the physical devices are grouped together to form functional units.
Typically this level contains some registers internal to the CPU (Central Processing Unit) and a data path containing an arithmetic logic unit.
In each clock cycle, one or two operands are fetched from the registers and combined in the arithmetic logic unit 
(for example, by addition or Boolean AND) .
The result is stored in one or more registers.
On some machines, the operation of the data path is controlled by software, called \kw{microprogram}.
On the other machines, it is controlled directly by hardware circuits.

The purpose of the data path is to execute some set of instructions.
Some of these can be carried out in one data path cycle; others may require multiple data path cycles.
These instructions may use registers or other hardware facilities.
Together, the hardware and instructions visible to an assembly language programmer form the \kw{ISA (Instruction Set Architecture)}.
This level is often called \kw{machine language}.

The machine language typically has between 50 and 300 instructions, mostly for moving data around the machine, doing arithmetic, and comparing values.
In this level, the input/output devices are controlled by loading the values into special \kw{device registers}.
For example, a disk can be commanded to read by loading the values of the disk address, main memory address, byte count, 
and direction (read or write) into its registers.
In practice, many more parameters are needed, and the status returned by the device after an operation may be complex.
Furthermore, for many I/O (Input/Output) devices, timing plays an important role in the programming.

A major function of the operating system is to hide all this complexity and give the programmer a more conveninent set of instructions to work with.
For example, read block from file is conceptually much simple than having to worry about the details of moving disk heads, 
waiting for them to settle down, and so on.

On top of the operating system is the rest of the system software.
Here we find the command interpreter (shell), window system, compilers, editors, and similar application-independent programs.
It is important to realize that these programs are definitely not part of the operating system, 
even though they are typically supplied preinstalled by the computer manufacturer, 
or in a package with the operating system if it is installed after purchase.
This is a crucial, but subtle, point.
The operating system is (usaully) that portion of the software that run in \kw{kernel mode} or \kw{supervisor mode}.
It is protected from user tampering by the hardware 
(ignoring for the moment some older or low-end microprocessors that do not have hardware protection at all).
Compilers and editors run in \kw{user mode}.
If a user does not like a particular compiler, he is free to write his own if he so chooses; 
he is not free to write his own clock interrupt handler, which is part of the operating system 
and is normally protected by hardware against attempts by user to modify it.

This distinction, however, is sometimes blurred in embedded systems (which may not have kernel mode) 
or interpreted systems (such as Java-based systems that use interpretation, not hardware , to seperate the components).
Still, for traditional computers, the operating system is what runs in kernel mode.

That said, in many systems there are programs that run in user mode but which help the operating sytem or perform privileged functions.
For example, there is often a program that allows users to change their passwords.
This program is not part of the operating system and does not run in kernel mode, but it clearly carries out a sensitive function 
and has to be protected in a specail way.

In some systems, including MINIX 3, this idea is carried to an extreme form, 
and pieces of what is traditionally considered to be the operating system (such as the file system) run in user space.
In such systems, it is difficult to draw a clear boundary.
Everything runing in kernel mode is clearly part of the operating system, 
but some programs running outside it are arguably also part of it, or at least closely associated with it.
For example, in MINIX 3, the file system is simply a big C program running in uer-mode.

Finally, above the system programs come the application programs.
These programs are purchased (or written by) the user to solve their particular problems, 
such as word processing, spreadsheets, engineering calculations, or storing information in a database.

\section{What Is an Operating System?}
Most computer users have had some experience with an operating system, but it is difficult to pin down precisely what an operating system is.
Part of the problem is that operating systems perform two basically unreleated functions, extending the machine and managing resources, 
and depending on who is doding the talking, you hear mostly about one function or the other.
Let us now look at both.

\subsection{The Operating System as an Extended Machine}
As mentioned earlier, the architecture (instruction set, memory organization, I/O, and bus structure) of most computers 
at the machine language level is primitive and awkward to program, especially for input/output.
To make this point more concrete, let us briefly look at how floppy disk I/O is done 
using the NEC PD765 compatible controller chips used on many Intel-based personal computers.
(Throughout this book we will use the term "floppy disk" and "diskette" interchangeably.)
The PD765 has 16 commands, each specified by loading between 1 and 9 bytes into a device register.
Those commands are for reading and writing data, moving the disk arm, and formatting tracks, 
as well as initializing, sensing, resetting, and recalibrating the controller and the drives.

The most basic commands are read and write, each of which requires 13 parameters, packed into 9 bytes.
These parameters specify such items as the address of the disk block to be read, the number of sectors per track, 
the recording mode used on the physical medium, the intersector gap spacing, and what to do with a deleted-data-address-mark.
If you do not understand this mumbo jumbo, do not worry; that is precisely the pointit is rather esoteric.
When the operation is completed, the controller chip returns 23 status and error fields packed into 7 bytes.
As if this were enough, the floppy disk programmer must also be constantly aware of whether the motor is on or off.
If the motor is off, it must be turned on (with a long startip delay) before data can be read or write.
The motor cannot be left on too long, however, or the floppy disk will wear out.
The programmer is thus forced to deal with the trade-off between long startup delays versus wearing out floppy disks (and losing the data on them).

Without going the real details, it should be clear that the average programmer probably does not want to 
get too intimately involved with the programming of floppy disks (or hard disks, which are just as complex and quite different).
Instead, what the programmer wants is a simpple, highlevel abstraction to deal with.
In the case of disks, a typical abstraction would be that the disk contains a collection of named files.
Each file can be opened for reading or writing, then read or written, and finally closed.
Details such as whether or not recording should use modified frequency modulation and 
what the current state of the motor is should not appear in the abstraction presented to the user.

The program that hides the truth about the hardware from the programmer and 
presents a nice, simple view of named files can be read and written is, of course, the operating system.
Just as the operating system shields the programmer from the disk hardware and presents a simple file-oriented interface, 
it also conceals a lot of unpleasant business concerning interrupts, timers, memory management, and other low-level features.
In each case, the abstraction offered by the operating system is simpler and easier to use than that offered by the underlying hardware.

In this view, the function of the operating system is to present the user with the equivalent of an \kw{extended machine} or \kw{virtual machine} 
that is easier to program than the underlying hardware.
How the operating system achives this goal is a long story, which we will study in detail throuout this book.
To summarize it in a nutshell, the operating system provides a variety of services 
that programs can obtain using special instructions called system calls.
We will examine some of the more common system calls later in this chapter.

\subsection{The Operating System as a Resource Manager}
The concept of the operating system as primarily providing its users with a convient interface is a top-down view.
An alternative, bottom-up, wiew hols that the operating system is there to manage all the pieces of a complex system. 
Modern computers consist of processors, memories, timers, disks, mice, network interfaces, printers, and a wide variety of other devices.
In the alternative view, the job of the operating system is to provide for an orderly and controlled allocation 
of the processors, memories, and I/O devices among the various programs competing for them.

Imagine what would happen if three programs running on some computer all tried to print their output simultaneously on the same printer.
The first few lines of printout might be from program 1, the next few from program 2, then some from program 3, and so forth.
The result would be chaos.
The operating system can bring order to the potential chaos by buffering all the output destined for the printer on the disk.
When one program is finished, the operating system can then copy its output from the disk file where it has been stored to the printer, 
while at the same time the other program can continue generating more output, 
oblivious to the fact that the output is not really going to the printer (yet).

When a computer (or network) has multiple users, the need for managing and protecting the memory, I/O devices, and other resources is even greater, 
since the users might otherwise interface with one another.
In addition, users often need to share not only hardware, but information (files, databases, etc.) as well.
In short, this view of the operating system holds that its primary task is to keep track of who is using which resource, 
to grant resource requests, to account for usage, and to mediate conflicting requests from different program and users.

Resource management includes multiplexing (sharing) resources in two ways: in time and in space.
When a resource is time multiolexed, different programs or users turns using it.
First one of them gets to use the resource, then another, and so on.
For example, with only one CPU and multiple programs that want to run on it, the operating system first allocates the CPU to one program, 
then after it has run long enough, another one gets to use the CPU, then another, and then eventually the first one again.
Determining how the resource is time multiplexed, who goes nexti,  and for how long is the task of the operating system.
Another example of time multiplexing is sharing the printer.
When multiple print jobs are queued up for printing on a single printer, a decision has to be made about which one is to be printed next.

The other kind of multiplexing is space multiplexing. 
Instead of the customers taking turns, each one gets part of the resource.
For example, main memory is normally divided up among several running programs, so each one can be resident at the same time 
(for example, in order to take turns using the CPU).
Assuming there is enough memory to hold multiple programs, 
it is more efficient to hold several programs in memory at once rather than give one of them all of it, 
especially if it only needs a small fraction of the total.
Of course, this raises issues of fairness, protection, and so on, and it is up to the operating system to solve them.
Another resource that is space multiplexed is the (hard) disk.
In many systems a single disk can hold files from many users at the same time.
Allocating disk space and keeping track of who is using which disk blocks is a typical operating system resource management task.

\section{History of Operating Systems}
Operating systems have been evolving through the years.
In the following sections we will breifly look at a few of the highlights.
Since operating systems have historically been closely tied to the architecture of the computers on which they run, 
we will look at successive generations of computers to see what their operating systems were like.
This mapping of operating system generations to computer generations is crude, but it dose provide some structure where there would otherwise be none.

The first true digital computer was designed by the English mathematician Charles Babbage(1792-1871).
Although Babbage spent most of his life and fortune trying to build his "analytical engine", 
he never got it working properly because it was purely mechanical, 
and the technology of his day could not produce the required wheels, gears, and cogs to the high precision that he needed.
Needless to say, the analytical engine did not have an operating system.

As an intersting historical aside, Babbage realized that he would need software for his analytical engine, 
so he hired a young woman naned Ada Lovelace, who was the daughter of the famed British poet Lord Byron, as the world's first programmer.
The programming language Ada was named after her.

\subsection{The First Generation(195565) Vacuum Tubes and Plugboards}
After Babbage's unsuccessful efforts, little progress was made in constructing digital computers untill World War II.
Around the mid-1940s, Howard Aiken at Harvard University, John Von Neumann at the Institute for Advanced Study in Princeton, 
J. Presper Eckert and John Mauchley at the University of Pennsylvania, and Konrad Zuse in Germany, 
among others, all succeeded in building caculating engines.
The first ones used mechanical relays but were very slow, with cycle times measured in seconds.
Relays were later replaced by vacuum tubes.
These machines were enormous, filling up entire rooms with tens of thousands of vacuum tubes, 
but they were still millions of times slower than even the cheapest personal computers available today.

In these early days, a single group of people designed, built, programmed, operated, and maintained each machine.
All programming was done in absolute machine language, often by wiring up plugboards to control the machine's basic functions.
Programming labguages were unknown (even assembly language was unknow).
Operating systems were unheard of.
The usual mode of operation was for the programmer to sign up for a block of time on the signup sheet on the wall, 
then come down to the machine room, insert his or her plugboard into the computer, 
and spend the next few hours hoping that none of the 20,000 or so vacuum tubes would burn out during the run.
Virtually all the problems were straitforward numerical calculations, such as grinding out tables of sines, cosines, and logarithms.

By the early 1950s, the routine had improved somewhat within the introduction of punched cards.
It was now possible to write programs on card and read them in instead of using plugboards; otherwise, the procedure was the same.

\subsection{The Second Generation(195565) Transistors and Batch Systems}
The introduction of the transistor in the mid-1950s changed the picture radically.
Computers became reliable enough that they could be manufactured and sold to paying customers with the expectation 
that they would continue to function long enough to get some useful work done.
For the first time, there was a clear separation between designers, builders, operators, programmers, and maintenance personnel.

These machines, now called \kw{mainframes}, were locked away in specially airconditioned computer rooms, 
with staffs of special-trained professional operators to run them.
Only big corporation or major government agencies or universities could afford their multimillion dollar price tags.
To run a \kw{job}(i.e., a program or set of programs), a programmer would first write the program on paper 
(in FORTRAN or possibly even in assembly language), then punch it on cards.
He would then bring the card deck down to the input room and hand it to one of the operators and go drink coffee untill the output was ready.

When the computer finished whatever job it was currently running, an operator would go over to the printer and tear off the output i
and carry it over to the output-room, so that the programmer could collect it later. 
Then he would take one of the card decks that had been brought from the input room and read it in.
If the FORTRAN compiler was needed, the operator would have to get it from a file cabinet and read it in.
Much computer time was wasted while operators were walking around the machine room.

Given the high cost of the equipment, it is not surprising that people quickly looked for ways to reduce the wasted time.
The solution generally adopted was the \kw{batch system}.
The idea behind using a small (relatively) inexpensive computer, such as the IBM 1401, 
which was very good at reading cards, copying tapes, and printing output, but not at all good at numerical calculations.
Others, much more expensive machines, such as the IBM 7094, were used for the real computing.
THis solution is show in Fig.1-2.

After about an hour of collecting a batch of jobs, the tape was rewound and brought into the machine room, where it was mounted on a tape drive.
The operator then loaded a special program (the ancestor of today's operating system), which read the first job from tape and ran it.
The output was written onto a second tape, instead of being printed.
After each job finished, the operating system automatically read the next job from the tape and began running it.
When the whole batch was done, the operator removed the input and output tapes, replaced the input tape with the next batch, 
and brought the output tape to a 1401 for printing \kw{off line} (i.e., not connected to the main computer).

The structure of atypical input job is show in Fig.1-3.
It started out with a \$JOB card, specifying the maximum run times in minutes, the account number to be charged, and the programmer's name.
Then came a \$FORTRAN card, telling the operating system to load FORTRAN compiler from the system tape.
It was followed by the program to be compiled, and then a \$LOAD card, directing the operating system to load the object program just compiled.
(Compileed programs were often written on scratch tapes and had to be loaded explicitly.)
Next came the \$RUN card, telling the operating system to run the program with the data following it.
Finally, the \$END card marked the end of the job.
These primitive control cards were the forerunners of modern job control language and command interpreters.

Large second-generation computers were used mostly for scientific and engineering calculations, 
such as solving the partial differential equations that often occur in physics and engineering.
They were largely programmed in FORTRAN and assembly labguage.
Typical operating systems were FMS (the Fortran Monitor System) and IBSYS, IBM'S operating system for the 7094.

\subsection{The Third Generation (19651980) ICs and Multiprogramming}
By the early 1960s, most computer manufactures had two distinct, and totally incompatible, product lines.
On the one hand there were the word-oriented, large-scale scientific computers, such as the 7094, 
which were used for numerical caculations in science and engineering.
On the other hand, there were the character-oriented, commercial computers, such as the 1401, 
which were widely used for tape sorting and printing by banks and insurance companies.

Developing, maintaining, and marketing two completely different product lines was expensive proposition for the computer manufactures.
In addition, many new computer customers initially needed a small machine but later outgrew it and wanted a bigger machine 
that had the same architectures as their current one so it could run all their old programs, but faster.

IBM attempted to solve both of these problems at a single stroke by introducing the System/360.
The 360 was a series of software-compatible machines ranging from 1401-size to much more powerful than the 7094.
The machines differed only in price and performance (maximum memory, processor speed, number of I/O devices permitted, and so forth).
Since all themachines had the same architecture and instruction set, programs written for one machine could run on all the others, at least in theory.
Furthermore, the 360 was designed to handle both scientific (i.e., numerical) and commertial computing.
Thus a single family of machines could satisfy the needs of all customers.
In subsequent years, IBM has come out with compatible successors to the 360 line, using more modern technology, 
known as the 370, 4300, 3080, 3090, and Z series.

The 360 was the first major computer line to use (small-scale) Integrated Circuits (ICs), 
thus providing a major price/performance advantage over the second-generation machines, which were built up from individual transistors.
It was an immediate success, and the idea of a family of compatible cpmputers was soon adopted by all the other major manufactures.
The descendants of these machines are still use at computer centers today.
Nowadays they are often used for managing huge databases (e/g., for airline reservation systems) 
or as servers for World Wide Web sites that must process thousands of requests per second.

The greatest strenth of the "one family" idea was simultaneously its greatest weakness.
The intention was that all software, including the operating system, \kw{OS/360}, had to work on all models.
It had to run on samll system, which often just replaced 1401s for copying cards to tape, 
and on very large systems, which often replaced 7094s for doing weather forecasting and other heavy computing.
It had to be good on systems with few peripherals and on systems with many peripherals.
It had to work in commercial environments and in scientific environments.
Above all, it had to be efficient for all of these different uses.

There was no way that IBM(or anybody else) could write a piece of software to meet all those conflicting requirements.
The result was an enormous and extraordinarily complex operating system, probably two to three orders of magnitude larger than FMS.
It consisted of millions of lines of assembly language written by thousands of programmers, 
and contained thousands upon thousands of bugs, which necessitated a continuous stream of new releases in an attempt to correct them.
Each new release fixed some bugs and introduced new ones, so the number of bugs probably remained constant in time.

One of the designers of OS/360, Fred Brooks, subsequently wrote a witty and incisive book describing his experiences with OS/360 (Brooks, 1995).
While it would be impossible to summarize the book here, suffice it to say that the cover shows a herd of prehistoric beasts stuck in a tar pit.
The cover of Silberschatz et al.(2004) makes a similar point about operating systems being dinosaurs.

Despite its enormous size and problems, OS/360 and the similar third-generation operating systems produced by other computer manufacturers 
actually satisfied most of their customers reasonably well.
They also popularized several key techniques absent in second-generation operating systems.
Probably the most important of these was \kw{multiprogramming}.
On the 7094, when the current job paused to wait for a tape or other I/O operation to complete, the CPU simply sat idle untill the I/O finished.
With heavily CPU-bound scientific caculations, I/O is infrequent, so this wasted time is not significant.
WIth commercial data processing, the I/O wait time can often be 80 or 90 oercent of the total time, 
so something had to be done to avoid having the (expensive) CPU be idle so much.

The solution that evolved was to partition memory into several pieces, with different job in each partition, as shown in Fig. 1-4.
While one job was waiting for I/O to complete, another job could be using the CPU.
If enough jobs could be held in main memory at once, the CPU could be kept busy nearly 100 percent of the time.
Having multiple jobs safely in memory at onece requires special hardware to protect each job against snooping and mischief by the other ones,
but the 360 and other third-generation systems were equipped with this hardware.

Another major feature present in third-generation operating systems was the ability to read jobs from cards onto the disk 
as soon as they were brought to the computer room.
Then, whenever a running job finished, the operating system could load a new job from the disk into the now-empty partion and run it.
This technique is called \kw{spooling} (from Simultaneous Peripheral Operation On Line) and was used for output.
With spooling, the 140qs were no longer needed, and much carrying of tapes disappeared.

Although third-generation operating systems were well suited for big scientific caculations and massive commercial data processing runs, 
they were still basically batch systems.
Many programmers pined for the first-generation days when they had the machine all to themselves for a few hours, 
so they could debug their programs quickly.
With third-generation systems, the time between submitting a job and getting back the output was often hours, 
so a single misplaced comma could cause compilation to fail, and the programmer to waste half day.
The desire for quick response time paved the way for \kw{timesharing}, a variant of multiprogramming, inwhich each user has an online terminal.
In a timesharing system, if 20 users are logged in and 17 of them are thinking or talking or drinking coffee, 
the CPU can be allocated in turn to the three jobs that want service.
Since people debugging programs usually issue short commands (e.g., compile a five-page procesure) 
rather than long ones (e.g., sort a million-record file), the computer can provide fast, 
interactive service to a number of users and perhaps also work on big batch jobs in the backgroud when the CPU is otherwise idle.
The first serious timesharing system, \kw{CTSS} (Compatible Time Sharing System), was developed at M.I.T.
In a special modified 7094 (Corbato et al., 1962). 
However, timesharing did not really become popular until the necessary protection hardware became widespread during the third generation. 

After the success of the CTSS system, MITM Bell Labs, and General Electric (then a major computer manufacturer) decided to embark on 
the development of a "computer utility", a machine that would support hundreds of simultaneous timesharing users.
Their model was the electricity distribution system:
when you need electric power, you just stick a plug in the wall, and within reason, as much power as you need will be there.
The designer of this system, known as \kw{MULTICS} (MULTiplexed Information and Computing Service), 
envisioned one huge machine providing computing power for everyone in the Boston area.
The idea that machines far more powerful than their GE-645 mainframe wwould be sold for under a thousand dollars by the millions only 30 years later 
was pure science fiction, like the idea of supersonic trans-Atlantic underse a trains would be now.

MULTICS was a mixed success.
It was designed to support hundreds of users on a machine only slightly more powerful than an Intel 80386-base PC,
although it had much more I/O capacity.
This is not quite as crazy as it sounds, since people knew how to write small, efficent programs in those days, a skill that has subsequntly been lost.
There were many reasons that MULTICS did bot take over the world, not the least of which is that it was written in PL/I, 
and the PL/I compiler was years late and barely worked at all when it finally arrived.
In addition, MULTICS was enormously ambitious for its time, much like Charles Babbage's analytical engine in the nineteenth century.

MULTICS introduced many seminal ideas into the computer literature, 
but turning it into a serious products and commercial success was a lot harder than anyone had expected.
Bell Labs dropped out of the project, and General Electric quit the computer business altogether.
However, M.I.T. persisted and eventually got MULTICS working.
It was ultimately sold as a commertial product by the company that bought GE's computer business (Honeywell) 
and installed by about 80 major companies and universities worldwide.
While their numbers were small, MULTICS users were fiercely loyal.
General Motors, Ford, and the U.S. National Security Agency, for example, only shut down their MULTICS systems in the late 1990s.
The last MULTICS running, at the Canadian Department of National Defence, shut down in October 2000.
Despite its lack of commertial success, MULTICS had huge influence on subsequent operating systems.
A great deal of information about it exists (Corbato et al., 1972; Corbato and Vyssotsky, 1965; Daley and Dennis, 1968; Organick, 1972l and Saltzerm 1974).
It also has a stillactive Web site, \www{www.multicians.org}, with a great deal of information about the system, its designers, and its users.

The phrase "computer utility" is no longer heard, but the idea has gained new life in recent years.
In its simplest form, PCs or workstation (high-end PCs) in a business or a classromm 
may be connected via a \kw{LAN (Local Area Network)} to a \kw{file server} on which all programs and data are stored.
An administrator then has to install an protect only one set of programs and data, 
and can easily reinstall local software on a malfunctioning PC or workstation without worrying about retrieving or preserving local data.
In more heterogeneous environments, a class software called \kw{middleware} has evolved 
to bridge the gap betwwen local users and the files, programs, and databases they use on remote servers.
Middleware makes networked computers look local to individual user' PCs or workstations 
and presents a consistent user interface even though there may be a wide variety of different servers, PCs, and workstations in use.
The World Wide Web is an example.
A web brower presents documents to a user in a uniform way, and a document as seen on a user's brower can consist of next from one server 
and graphics from another server, presented in a format determined by a style sheet on yet another sever.
Businesses and universities commonly use a web interface to access databases and run programs on a computer in another building or even another city.
Middleware appears to be the operating system of a \kw{distributed system}, 
but it is not really an operating system at all, and is beyond the scope of this book.
For more on distributed systems see Tanenbaum and Van Steen (2002).

Another major development during the third generation was the phenomental growth of minicompputers, 
starting with the Digital Equipment Company (DEC) PDP-1 in 1961.
The PDP-1 had only 4K of 18-bit words, but at \$120,000 per machine (less than 5 percent of the price of a 7094), it sold like hotcakes.
For certain kinds of nonnumerical work, it was almost as fast as the 7094 and gave birth to a whole new industry.
It was quickly followed by a series of other PDPs (unlike IBM's family all incompatible) culminating in the PDP-11.

One of the computer scientists at Bell Labs who had worked on the MULTICS project, Ken Thompson, 
subsequently found a small PDP-7 minicomputer that no one was using and set out to write a stripped-down, one-user version of MULTICS.
This work later developed into the \kw{UNIX} operating system, which became popular in the academic world, 
with government agencies, and with many companies.

The history of UNIX has been told elsewhere (e.g., Salus, 1994).
Because the source code was widely available, various organizations developed their own (incompatible) versions, which lead to chaos.
Two major versions developed, \kw{System V}, from AT\&T, and \kw{BSD}, (Berkeley Software Distribution) from the University of California at Berkeley.
These had minor variants as well, now including FreeBSD, OpenBSD, and NetBSD.
To make it possible to write programs that could run on any UNIX system, IEEE developed a standard for UNIX, called \kw{POSIX},
that most versions of UNIX now support.
In fact, some other operating system now also support the POSIX interface.
The information needed to write POSIX-compliant software is available in books (IEEE, 1990; Lewine, 1991),
and online as the Open Group's "Single UNIX Specification" at \www{www.unix.org}.
Later in this chapter, when we refer to UNIX, we mean all of these systems as well, unless stated otherwise.
While they differ internally, all of them support the POSIX X standard, so to the programmer they are quite similar.

\subsection{The Fourth Generation (1980Present) Personal Computers}
With the development of LSI (Large Scale Integration) circuits, chips containing thousands of transistors on a square centimeter of silicon,
the age of the \kw{microprocessor}-based personal computer dawned.
In terms of architecture, personal computers (initially called \kw{microcomputers}) were not all that different from minicomputers of the PDP-11 class, 
but in terms of price they vertainly were different.
The minicomputer made it possible for a department in a cpmpany or university to have its own computer.
The microcomputer made it possible for an individual to have his or her own computer.

There were several families of microcomputers.
Intel came out with the 8080, the first general-purpose 8-bit microprocessor, in 1974.
A number of companies produced complete systems using the 8080 (or the compatible Zilog Z80) 
and the \kw{CP/M} (Control Program for Microcomputers) operating system from a company called Digital Research was widely used with these.
Many application programs were written to run on CP/M, and it dominated the personal computing world for about 5 years.

Motorola also produced an 8-bit microprocessor, the 6800.
A group of Motorola engineers left to form MOS Technology and manufacture the 6502 CPU after Motorola rejected their suggested improvements to the 6800.
The 65-2 was the CPU of several early systems.
One of these, the Apple II, became a major competitor for CP/M systems in the home and educational markets.
But CP/M was so popular that many owners of Apple II computers purchased Z-80 coprocessor add-on cards to run CP/M, 
since the 6502 CPU was not compatible with CP/M.
The CP/M cards were sold by little company called Microsoft, which also had a market niche supplying BASIC interpreters 
used by a number of microcomputers running CP/M.

The next generation of microprocessors were 16-bit systems.
Intel came out with the 8086, and in the early 1980s, IBM designed the IBM PC around Intel's 8088 (an 8086 on the inside, with an 8bit external data path).
Microsoft offered IBM a package which included Microsoft's BASIC and an operating system, 
\kw{DOS} (Disk Operating System) originally developed by another company.
Microsoft bought the product and hired the original author to improve it.
The revised system was renamed \kw{MS-DS} (MicroSoft Disk Operating System) and quickly came to dominate the IBM PC market.

CP/M, MS-DOS, and the Apple DOS were all command-line systems: user typed commands at the keyboard.
Years earlier, Doug Engelbart at Stanford Research Institute had invented the \kw{GUI (Graphical User Interface)}, pronounced "gooey", 
complete with windows, icons, menus, and mouse.
Apple's Steve Jobs saw the possibility of a truly \kw{user-friendly} personal computer 
(for users who knew nothing about computers and did not want to learn), and the Apple Macintosh was announced in early 1984.
It used Motorola's 16-bit 68000 CPU, and had 64KB of \kw{ROM (Read Only Memory)}, to support the GUI.
The Macintosh has envolved over the years.
Subsequent Motorola CPUs were true 32-bit systems, and later still Apple moved to IBM PowerPC CPUs, with RISC 32-bit (and later, 64-bit) architecture.
In 2001 Apple made a major operating system change, releasing \kw{Mac OS X}, with a new version of the Macintosh GUI on top of Berkeley UNIX.
And in 2005 Apple announced that it would be switching to Intel processors.

To compete with the Macintosh, Microsoft invented Windows.
Originally Windows was just a graphical environment on top of 16-bit MS-DOS (i.e., it was more like a shell than a true operating system).
However, current versions of Windows are descendant of Windows NT, a full 32-bit system, rewritten from scratch.

The other major contender in the personal computer world is UNIX (and its various derivatives).
UNIX is strongest on workstations and other high-end computers, such as network servers.
It is especially popular on machines powerd by high-performance RISC chips.
On Pentium-based computers, Linux is becoming a popular alternative to Windows for students and increasingly many corporate users.
(Throughout this book we will use the term "Pentium" to means the entire Pentium family, 
including the low-end Celeron, the gigh end Xeon, and compatible AMD microprocessors).

Although many UNIX users, especially experienced programmers, prefer a command-based interface to a GUI, 
nearly all UNIX systems support a windowing system called \kw{X Window} system developed at M.I.T.
This system handles the basic window management, allowing users to create, delete, move, and resize windows using a mouse.
Often a complete GUI, such as \kw{Motif}, is available to run on top of the X Window system 
giving UNIX a look and feel something like the Macintosh or Microsoft Windowns for those UNIX users who want such a thing.

An intresting development that began taking place during the mid-1980s is the growth of 
networks of personal computers running \kw{network operating systems} and \kw{distributed operating systems} (Tanenbauand Van Steen, 2002).
In a network operating system, the users are aware of the existence of multiple computers 
and can log in to remote machines and copy files from one machine to another.
Each machine runs its own local operating system and has its own local user (or users).
Basically, the machines are independent of one another.

Network operating systems are not fundamentally different from single-processor operating systems.
They obviously need a network interface controller and some low-level software to drive it, 
as well as programs to achieve remote login and remote file access, 
but these additions do not change the essential structure of the operating system.

A distributed operating system, in contrast, is one that appears to its users as a traditional uniprocessor system, 
even though it is composed of multiple processors.
The users shpuld not be aware of where their programs are being run or where their files are located;
that should all be handled automatically and efficently by the operating system.

True distributed operating systems require more than just adding a little code to a uniprocessor operating system, 
because distributed and centralized systems differ in critical ways.
Distributed system, for example, often allow applications to run on several processors at the same time, 
thus requiring more complex processor scheduling algorithms in order to optimize the amount of parallelism.

Communication delays within the network often mean that these (and other) algorithms must run with incomplete, outdated, or even incorrect information.
This situation is radically different from a single-processor system in which the operating system has complete information about the system state.


\subsection{History of MINIX 3}
When UNIX was young (Version 6), the source code was widely available, under AT\&T license, and frequently studied.
John Lions, of the University of New South Wales in Australia, even wrote a little booklet describing its operation, line by line (Lions, 1996).
This booklet was used (with permission of AT\&T) as atext in many university operating system courses.

When AT\&T released Verion 7, it dimly began realize that UNIX was a valuable commercial product, 
so it issued Version 7 with a license that prohibited the source code from being studied in courses, 
in order to avoid endangering its status as a trade secret. 
Many universities complied by simply dropping the study of UNIX and teching only theory.

Unfortunately, teaching only theory leaves the student with a lopsided view of what an operating system is really like.
The theoretical topics that are usually covered in great detail in courses and books on operating systems, 
such as scheduling algorithms, are in practice not really that important. 
Subjects that really are important, such as I/O and file systems, are generally neglected because there is little about them.

To remedy this situation, one of the authors of this book (Tanenbaum) decided to write a new operating system from scratch 
that would be compatible with UNIX from the user's point of view,  but completely different on the inside.
By not using even one line of AT\&T code, this ystem avoided the licensing restrictions, so it could be used for class or individual study.
In this manner, readers could dissect a real operating system to see what is inside, just as biology students dissect frogs.
It was called \kw{MINIX} and was released in 1987 with its complete source code for anyone to study or modify.
The name MINIX stands for mini-UNIX because it is small enough that even a nonguru can understand how it works.

In additionto the advantage of eliminating the legal problems, MINIX had another advantage over UNIX.
It was written a decade after UNIX and was structured in a more modular way.
For instance, from the very first release of MINIX the file system and the memory manager 
were not part of the operating system at all but as user programs.
In the current release (MINIX 3) this modularization has been extended to the I/O device drivers, 
which (with the exception of the clock driver) all run as user programs.
Another difference is that UNIX was designed to be efficient; MINIX was designed to be readable 
(inasmuch as one can speak of any program hundreds of pages long as being readable).
The MINIX code, for example, has thousands of comments in it.

MINIX was originally designed for compatibility with Version 7 (V7) UNIX. Version 7 was used as the model because of its simplicity and elegance.
It is cometimes said that Version 7 was an improvement not only over all its predecessors, but also over all its successors.
With the advent of POSIX, MINIX began evolving toward the new standard, while maintaining backward compatibility with existing programs.
This kind of evolution is common in the computer industry, as no vendor wants to introduce a new system 
that none of its existing customers can use without great upheaval.
The version of MINIX described in this book, MINIX 3, is based on the POSIX standard.

Like UNIX, MINIX was written in the C programing language and intented to be easy to port to various computers.
The initial implementation was for the IBM PC.
MINIX was subsequently ported to several other platforms.
In keeping with the "Small is Beautiful" philosophy, MINIX originally did not even require a hard disk to run 
(in the mid-1980s hard disks were still an expensive novelty).
As MINIX grew in functionality and size, it eventually got the point that a hard disk was needed for PCs,
but in keeping with the MINIX philisophy, a 200-MB partion is sufficient (for embedded applications, no hard disk is reuired ehough).
In contrast, even small Linux systems require 500-MB of disk space, and several GB will needed to install common applications.

To the average user sitting at an IBM PC, running MINIX is similar to runing UNIX.
All of the basic programs, such as \sys{cat, grep, ls, make}, and the shell are present and perform the same functions as their UNIX counterparts. 
Like the operating system itself, all these utility programs have been rewritten completely from scratch by the author, 
his students, and some other dedicated people, with no AT\&T or other proprietary code.
Many other freely-distributable programs now exist, and in many cases these have been successfully ported (recompiled) on MINIX.

MINIX continued to develop for a decade and MINIX2 was released in 1997, together with the second edition of this book, which described the new release.
The changes between Versions 1 and 2 were substantial (e.g., from 16-bit real mode on an 8088 using floppy disks 
to 32-bit protected mode on a 386 using a hard disk) but evolutionary.

Development continued slowly but systematically until 2004, when Tanenbaum became convinced the software was getting too bloated and unreliable
and decide to pick up the slightly-dormant MINIX thread again.
Together with his students and programmers at the Vrije Universiteit in Amsterdam, he produced MINIX 3,
a major redesign of the system, greatly restructuring the kernel, reducing its size, and emphasizing modularity and reliability.
The new version was intended both for PCs and embedded systems, where compactness, modularity, and reliability are crucial.
While some people in the group called for a completely new name, it was eventually decide to call it MINIX 3 
since the name MINIX was already well known.
By way of analogy, when Apple abandoned it own operating system, Mac OS 9 and replaced it with a variant of Berkeley UNIX, 
the name chosen was Mac OS X rather than APPLIX or something like that.
Similar fundamental changes have happened in the Windows family while retaining the Windows name.

The MINIX kernel is well under 4000 lines of executable code, compared to millions of executable lines of code for Windows, Linux, FreeBSD, and other operating systems.
Small kernel size is important because kernel bugs are far more devastating than bugs in user-mode programs and more code means more bugs.
One careful study has shown that the number of \sys{detected} bugs per 1000 executable lines of code varies from 6 to 16 (Basili and Perricone, 1984).
The actual number of bugs is probably much higher since the researchers could only count reported bugs, not unreported bugs.
Yet another study (Ostrand et al., 2004) showed that even after more than a dozen releases, on the average 6\% of all files contained bugs 
that were later reported and after a certain point the bug level tends to stabilize rather than go asymptotically to zero.
This result is supported by the fact that when a very simple, automated, model-checker was let loose on stable versions of Linux and Open BSD, 
it found hundreds of kernel bugs, overwhelmingly in device drivers (Chou et al., 2001; and Engler et al., 2001).
THis is the reason the device drivers were moved out of the kernel in MINIX 3; they can do less damage in user mode.

Throughout this book MINIX 3 will be used as an example.
Most of the comments about the MINIX 3 system calls, however (as opposed to comments abount the actual code), also apply to other UNIX systems.
This remark should be kept in mind when reading the text.

A few words about Linux and its relationship to MINIX may possibly be of interest to some reader.
Shortly after MINIX released, a USENET newgroup, \www{comp.os.minix}, was formed to discuss it.
Within weeks, it had 40,000 subscribers, most of whom wanted to add vast numbers of new features to MINIX 
to make it bigger and better (well, at least bigger).
Every day, several hundred of them offered suggestions, ideas, and frequently snippets of source code.
The author of MINIX was able to successfully resist this onslaught for several years, 
in order to keep MINIX clean enough for students to understand and small enough that it could run on computers that students could afford.
For people who thought little of MS-DOS, the existence of MINIX (with source code) as an alternative was even a reason fo finally go out and buy a PC.

One of these people was a Finnish student named Linus Torvalds.
Torvalds installed MINIX on his new PC and studied the source code carefully.
Torvalds wants to read USENET newsgroups (such as \www{comp.os.minix}) on his own PC rather than his university, 
but some features he needed were lacking in MINIX, so he wrote a program to do that, but soon discovered he needed a different terminal driver,
so he wrote that too.
Then he wanted to download and save postings, so he wrote a disk driver, and then a file system.
By Aug. 1991 he had produced a primitive kernel.
On Aug. 25, 1991, he announced it on \www{comp.os.minix}.
This announcement attracted other people to help him, and on March 13, 1994 Linux 1.0 was released.
Thus was Linux born.

Linux has become one of the notable successes of the \kw{open source} movement (which MINIX helped start).
Linux is challenging UNIX (and Windows) in many environments, partly because commodity PCs which support Linux are now available with performance that rivals the proprietary RISC systems required by some UNIX implementations.
Other open source software, notably the Apache Web server and the MySQL database, 
and the open source Perl and PHP programming labguages are often used together on Web servers and sometimes referred to by the acronym \kw{LAMP}.
For more on the history of Linux and open source software see DiBona et al. (1999), Moody (2001), and Naughton (2000).

\section{Operating System Concepts}
The interface between the operating system and the user programs is defined by the set of "extended instructions" that the operating system provides.
These extended instructions have been traditionally known as \kw{system call}, although they can be implemented in several ways.
To really understand what operating system do, we must examine this interface closely.
The calls available in the interface vary from operating system to operating system (although the underlying concepts tend to be similar).

We are thus forced to make a choice between (1) vague generalities ("operating systems have system calls for reading files")
and (2) sime specific system ("MINIX 3 has a \cmd{read} system call with three parameters: 
one to specify the file, one to tell where the data are to be put, and one to tell how many bytes to read").

We have chose the latter approach.
It's more work that way, but it gives more insight into what operating systems really do.
In Sec. 1.4 we will look closely at the basic system calls present in UNIX (including the various version of BSD), Linux, and MINIX 3.
For simplicity's sake, we will refer only to MINIX 3, but the corresponding UNIX and Linux system calls are based on POSIX in most cases.
Before we look at the actual system calls, however, it is worth taking a bird's-eye view of MINIX 3, 
to get a general fell for what an operating system is all about.
this overview applies equally well to UNIX and Linux, as mentioned above.

The MINIX 3 system calls fall roughly in two broad categories: those dealing with processes and those dealing with the file system.
We will now examine each of these in turn.

\subsection{Processes}
A key concept in MINIX 3, and in all operating systems, is the \kw{process}.
A process is basically a program in execution.
Associated with each process is its \kw{address space}, a list of memory locations from some minimum (usually 0) to some maximum,
which the process can read and write.
The address space contains the executable program, the program's data, and its stack.
Also associated with each process is some set of registers, including the program counter, stack pointer, and other hardware registers, 
and all the other information needed to run the program.
 
We will come back to the process concept in much more detail in Chap.2, but for the time being, 
the easiest way to get a good intuitive feel for a process is to think about multiprogramming systems.
Periodically, the operating system decides to stop running one process and start running another, for example, 
because the first one has had more than its share of CPU time in the past second.













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

